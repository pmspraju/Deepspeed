# Deepspeed
try deepspeed with pytorch

# VLLM
try vllm in Lambda AI gpu instance

---

```markdown
# ğŸ§  LLM Deployment Journey: AWS Lambda â†’ SageMaker â†’ GPU Inference with vLLM

This guide documents an end-to-end AI deployment pipeline explored in real-world development: from serverless AWS Lambda functions and SageMaker debugging, to full vLLM inference running on Lambda GPU Cloud with Dockerized DeepSeek 7B models.

---

## ğŸª Part 1: AWS Lambda Functions

> Automate model workflows or data orchestration using serverless Lambda functions.

### âœ… Highlights
- Used Lambda to trigger SageMaker jobs and S3 processes
- Attached IAM execution role with policies for:
  - Accessing S3 buckets
  - Initiating SageMaker actions
- Debugged a key-based SSH authentication error (`Permission denied (publickey)`)
  - Determined it was unrelated to Lambda â€” traced to local scp/SSH issues fixed later

---

## ğŸ§© Part 2: AWS SageMaker Deployment

> Deployed DeepSeek 7B model using DeepSpeed on a DJL container with SageMaker

### âœ… Steps
- Configured IAM:
  - Attached S3 read access to execution role
  - Verified trust relationships for SageMaker
- Structured model archive:
  ```
  model.tar.gz/
  â”œâ”€â”€ model.py
  â”œâ”€â”€ config.json
  â”œâ”€â”€ tokenizer.json
  â””â”€â”€ pytorch_model.bin
  ```
- Uploaded model to S3 and created `HuggingFaceModel` using SageMaker Python SDK
- Used appropriate container URI (DJL DeepSpeed)
- Debugged endpoint failures and container logs
  - Fixed `ModelError`, permission denials, and environment mismatches

---

## âš™ï¸ Part 3: Lambda GPU + vLLM Inference Server

> Run DeepSeek 7B inside a CUDA 12.8 Docker container with vLLM for local or remote API access

### ğŸ“ Folder Structure

```
vllm-server/
â””â”€â”€ app/
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ start.sh
    â””â”€â”€ deepseek-math-7b/
```

---

### ğŸ› ï¸ Build Docker Image

```bash
# From inside app/
sudo docker build -t vllm-cuda128 .
```

- Re-ran build if PyPI downloads failed
- Used `sudo` for Docker access on new instance
- Optional: pinned `xformers` or added retry logic

---

### ğŸš€ Run Container

```bash
sudo docker run --gpus all -p 8000:8000 \
  -v $(pwd)/deepseek-math-7b:/models \
  -e MODEL_PATH=/models \
  vllm-cuda128
```

- Mounts model directory into container at `/models`
- Starts vLLM API server on `0.0.0.0:8000`

---

### ğŸ§ª Test Inference from Instance

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models",
    "prompt": "Explain the chain rule",
    "max_tokens": 64
  }'
```

- Verified model ID by hitting:
  ```
  curl http://localhost:8000/v1/models
  ```
- Used returned `"id": "/models"` as model name

---

### ğŸŒ Enable Browser & Remote Access

1. Go to Lambda Cloud firewall
2. Open TCP port 8000
3. Access via browser:
   ```
   http://<your-ip>:8000/v1/models
   ```

---

### ğŸ“¦ Export & Transfer Docker Image

On instance:
```bash
sudo docker save vllm-cuda128 -o vllmDs7b.tar
sudo chown ubuntu:ubuntu vllmDs7b.tar
```

From local machine:
```bash
scp -i ~/Keys/L1A100.pem \
  ubuntu@<ip>:/home/ubuntu/vllm-server/app/vllmDs7b.tar \
  ~/Downloads/
```

Load into local Docker:
```bash
docker load -i ~/Downloads/vllmDs7b.tar
```

---

## ğŸ§  Summary

| Capability            | What You Implemented                                        |
|----------------------|-------------------------------------------------------------|
| ğŸš€ Serverless         | Lambda function triggering SageMaker/S3 securely           |
| ğŸ§° Cloud inference    | SageMaker + DeepSpeed containerization                      |
| ğŸ³ Portable runtime   | CUDA 12.8 Docker with vLLM + DeepSeek                       |
| ğŸŒ API integration    | Browser-accessible OpenAI-style API from Lambda GPU        |
| ğŸ›« Image transfer     | Docker save/load + secure scp download                      |

---
