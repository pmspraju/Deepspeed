/home/nachiketa/pytorch_env/bin/python /home/nachiketa/Documents/Workspaces/Deepspeed/aws/sagemaker/deepseekmath/quantizeGptq.py

INFO  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.
INFO  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.
[2025-05-25 00:29:27,820] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Torch version: 2.5.1+cu121
CUDA Available: True
CUDA Version: 12.1
GPU Count: 1
GPU Name: NVIDIA GeForce RTX 4080
INFO  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]
INFO  Loader: Auto dtype (native bfloat16): `torch.bfloat16`
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 53.79it/s]
INFO  Model: Loaded `generation_config`: GenerationConfig {
  "bos_token_id": 100000,
  "eos_token_id": 100001
}

INFO  Kernel: loaded -> `[]`
INFO  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`
INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=100001 (token='<｜end▁of▁sentence｜>').
WARN  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 60.5517578125.
INFO  Process: progress logs for `gptq` will be streamed to file: `gptq_log_agnes_time_05_25_2025_00h_29m_31s.log`
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.k_proj     | 0.00001985 | 1024        | 0.01000     | 0.539     | 3.081        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.v_proj     | 0.00000295 | 1024        | 0.01000     | 0.340     | 3.081        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.q_proj     | 0.00002787 | 1024        | 0.01000     | 0.330     | 3.081        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.o_proj     | 0.00000423 | 1024        | 0.01000     | 0.318     | 1.621        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | mlp.up_proj          | 0.00003751 | 1024        | 0.01000     | 0.339     | 2.282        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | mlp.gate_proj        | 0.00006645 | 1024        | 0.01000     | 0.340     | 2.282        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | mlp.down_proj        | 0.00000562 | 1024        | 0.01000     | 1.064     | 5.801        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.k_proj     | 0.00005187 | 1024        | 0.01000     | 0.322     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.v_proj     | 0.00001449 | 1024        | 0.01000     | 0.334     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.q_proj     | 0.00005531 | 1024        | 0.01000     | 0.324     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.o_proj     | 0.00000887 | 1024        | 0.01000     | 0.320     | 1.596        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | mlp.up_proj          | 0.00011060 | 1024        | 0.01000     | 0.346     | 2.258        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | mlp.gate_proj        | 0.00010949 | 1024        | 0.01000     | 0.347     | 2.258        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | mlp.down_proj        | 0.00638598 | 1024        | 0.01000     | 1.077     | 5.783        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.k_proj     | 0.00035380 | 1024        | 0.01000     | 0.326     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.v_proj     | 0.00018597 | 1024        | 0.01000     | 0.335     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.q_proj     | 0.00040798 | 1024        | 0.01000     | 0.328     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.o_proj     | 0.00001671 | 1024        | 0.01000     | 0.324     | 1.601        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | mlp.up_proj          | 0.00021604 | 1024        | 0.01000     | 0.345     | 2.257        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | mlp.gate_proj        | 0.00022287 | 1024        | 0.01000     | 0.348     | 2.257        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | mlp.down_proj        | 0.00002792 | 1024        | 0.01000     | 1.066     | 5.771        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.k_proj     | 0.00042073 | 1024        | 0.01000     | 0.324     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.v_proj     | 0.00023468 | 1024        | 0.01000     | 0.330     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.q_proj     | 0.00048230 | 1024        | 0.01000     | 0.325     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.o_proj     | 0.00002270 | 1024        | 0.01000     | 0.320     | 1.591        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | mlp.up_proj          | 0.00031908 | 1024        | 0.01000     | 0.346     | 2.256        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | mlp.gate_proj        | 0.00033495 | 1024        | 0.01000     | 0.350     | 2.256        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | mlp.down_proj        | 0.00005262 | 1024        | 0.01000     | 1.063     | 5.767        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.k_proj     | 0.00075838 | 1024        | 0.01000     | 0.326     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.v_proj     | 0.00039405 | 1024        | 0.01000     | 0.334     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.q_proj     | 0.00085058 | 1024        | 0.01000     | 0.327     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.o_proj     | 0.00002860 | 1024        | 0.01000     | 0.324     | 1.605        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | mlp.up_proj          | 0.00043644 | 1024        | 0.01000     | 0.346     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | mlp.gate_proj        | 0.00046541 | 1024        | 0.01000     | 0.349     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | mlp.down_proj        | 0.00008456 | 1024        | 0.01000     | 1.071     | 5.786        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.k_proj     | 0.00074804 | 1024        | 0.01000     | 0.324     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.v_proj     | 0.00043334 | 1024        | 0.01000     | 0.333     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.q_proj     | 0.00087107 | 1024        | 0.01000     | 0.327     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.o_proj     | 0.00003989 | 1024        | 0.01000     | 0.324     | 1.582        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | mlp.up_proj          | 0.00056745 | 1024        | 0.01000     | 0.346     | 2.255        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | mlp.gate_proj        | 0.00061597 | 1024        | 0.01000     | 0.347     | 2.255        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | mlp.down_proj        | 0.00014951 | 1024        | 0.01000     | 1.063     | 5.774        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.k_proj     | 0.00073265 | 1024        | 0.01000     | 0.322     | 2.909        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.v_proj     | 0.00040081 | 1024        | 0.01000     | 0.334     | 2.909        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.q_proj     | 0.00073878 | 1024        | 0.01000     | 0.324     | 2.909        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.o_proj     | 0.00008366 | 1024        | 0.01000     | 0.318     | 1.581        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | mlp.up_proj          | 0.00065809 | 1024        | 0.01000     | 0.355     | 2.262        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | mlp.gate_proj        | 0.00070560 | 1024        | 0.01000     | 0.357     | 2.262        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | mlp.down_proj        | 0.00020356 | 1024        | 0.01000     | 1.092     | 5.772        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.k_proj     | 0.00088242 | 1024        | 0.01000     | 0.323     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.v_proj     | 0.00050739 | 1024        | 0.01000     | 0.336     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.q_proj     | 0.00088916 | 1024        | 0.01000     | 0.334     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.o_proj     | 0.00010625 | 1024        | 0.01000     | 0.332     | 1.608        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | mlp.up_proj          | 0.00082043 | 1024        | 0.01000     | 0.351     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | mlp.gate_proj        | 0.00090248 | 1024        | 0.01000     | 0.351     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | mlp.down_proj        | 0.00031979 | 1024        | 0.01000     | 1.092     | 5.790        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.k_proj     | 0.00081696 | 1024        | 0.01000     | 0.334     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.v_proj     | 0.00045344 | 1024        | 0.01000     | 0.338     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.q_proj     | 0.00083452 | 1024        | 0.01000     | 0.335     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.o_proj     | 0.00014972 | 1024        | 0.01000     | 0.331     | 1.586        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | mlp.up_proj          | 0.00093403 | 1024        | 0.01000     | 0.345     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | mlp.gate_proj        | 0.00099719 | 1024        | 0.01000     | 0.348     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | mlp.down_proj        | 0.00041517 | 1024        | 0.01000     | 1.074     | 5.772        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.k_proj     | 0.00108919 | 1024        | 0.01000     | 0.329     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.v_proj     | 0.00065300 | 1024        | 0.01000     | 0.334     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.q_proj     | 0.00131218 | 1024        | 0.01000     | 0.328     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.o_proj     | 0.00020896 | 1024        | 0.01000     | 0.332     | 1.590        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | mlp.up_proj          | 0.00108924 | 1024        | 0.01000     | 0.352     | 2.269        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | mlp.gate_proj        | 0.00114857 | 1024        | 0.01000     | 0.355     | 2.269        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | mlp.down_proj        | 0.00052141 | 1024        | 0.01000     | 1.067     | 5.784        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.k_proj     | 0.00112453 | 1024        | 0.01000     | 0.325     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.v_proj     | 0.00072678 | 1024        | 0.01000     | 0.335     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.q_proj     | 0.00128847 | 1024        | 0.01000     | 0.333     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.o_proj     | 0.00023785 | 1024        | 0.01000     | 0.332     | 1.597        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | mlp.up_proj          | 0.00121168 | 1024        | 0.01000     | 0.344     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | mlp.gate_proj        | 0.00124881 | 1024        | 0.01000     | 0.347     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | mlp.down_proj        | 0.00065353 | 1024        | 0.01000     | 1.068     | 5.782        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.k_proj     | 0.00128270 | 1024        | 0.01000     | 0.324     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.v_proj     | 0.00083857 | 1024        | 0.01000     | 0.336     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.q_proj     | 0.00153403 | 1024        | 0.01000     | 0.335     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.o_proj     | 0.00029285 | 1024        | 0.01000     | 0.333     | 1.606        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | mlp.up_proj          | 0.00135331 | 1024        | 0.01000     | 0.346     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | mlp.gate_proj        | 0.00136664 | 1024        | 0.01000     | 0.349     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | mlp.down_proj        | 0.00074170 | 1024        | 0.01000     | 1.074     | 5.782        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.k_proj     | 0.00137634 | 1024        | 0.01000     | 0.325     | 2.927        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.v_proj     | 0.00087323 | 1024        | 0.01000     | 0.336     | 2.927        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.q_proj     | 0.00147735 | 1024        | 0.01000     | 0.336     | 2.927        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.o_proj     | 0.00031062 | 1024        | 0.01000     | 0.322     | 1.607        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | mlp.up_proj          | 0.00149069 | 1024        | 0.01000     | 0.347     | 2.278        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | mlp.gate_proj        | 0.00146746 | 1024        | 0.01000     | 0.349     | 2.278        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | mlp.down_proj        | 0.00088481 | 1024        | 0.01000     | 1.072     | 5.790        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.k_proj     | 0.00152959 | 1024        | 0.01000     | 0.325     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.v_proj     | 0.00107942 | 1024        | 0.01000     | 0.335     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.q_proj     | 0.00174280 | 1024        | 0.01000     | 0.327     | 2.923        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.o_proj     | 0.00037548 | 1024        | 0.01000     | 0.326     | 1.608        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | mlp.up_proj          | 0.00161538 | 1024        | 0.01000     | 0.346     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | mlp.gate_proj        | 0.00155030 | 1024        | 0.01000     | 0.350     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | mlp.down_proj        | 0.00104844 | 1024        | 0.01000     | 1.074     | 5.781        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.k_proj     | 0.00143200 | 1024        | 0.01000     | 0.324     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.v_proj     | 0.00100413 | 1024        | 0.01000     | 0.335     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.q_proj     | 0.00155328 | 1024        | 0.01000     | 0.330     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.o_proj     | 0.00064771 | 1024        | 0.01000     | 0.322     | 1.596        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | mlp.up_proj          | 0.00173733 | 1024        | 0.01000     | 0.346     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | mlp.gate_proj        | 0.00163646 | 1024        | 0.01000     | 0.353     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | mlp.down_proj        | 0.00129538 | 1024        | 0.01000     | 1.066     | 5.778        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.k_proj     | 0.00165415 | 1024        | 0.01000     | 0.326     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.v_proj     | 0.00142532 | 1024        | 0.01000     | 0.340     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.q_proj     | 0.00187119 | 1024        | 0.01000     | 0.326     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.o_proj     | 0.00063853 | 1024        | 0.01000     | 0.326     | 1.603        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | mlp.up_proj          | 0.00198716 | 1024        | 0.01000     | 0.349     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | mlp.gate_proj        | 0.00186254 | 1024        | 0.01000     | 0.350     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | mlp.down_proj        | 0.00169545 | 1024        | 0.01000     | 1.069     | 5.786        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.k_proj     | 0.00178392 | 1024        | 0.01000     | 0.327     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.v_proj     | 0.00159698 | 1024        | 0.01000     | 0.338     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.q_proj     | 0.00200559 | 1024        | 0.01000     | 0.328     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.o_proj     | 0.00063093 | 1024        | 0.01000     | 0.323     | 1.608        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | mlp.up_proj          | 0.00229918 | 1024        | 0.01000     | 0.348     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | mlp.gate_proj        | 0.00215838 | 1024        | 0.01000     | 0.348     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | mlp.down_proj        | 0.00212617 | 1024        | 0.01000     | 1.067     | 5.776        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.k_proj     | 0.00176116 | 1024        | 0.01000     | 0.325     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.v_proj     | 0.00154136 | 1024        | 0.01000     | 0.332     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.q_proj     | 0.00198934 | 1024        | 0.01000     | 0.324     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.o_proj     | 0.00091225 | 1024        | 0.01000     | 0.323     | 1.601        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | mlp.up_proj          | 0.00261287 | 1024        | 0.01000     | 0.347     | 2.264        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | mlp.gate_proj        | 0.00245481 | 1024        | 0.01000     | 0.349     | 2.264        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | mlp.down_proj        | 0.00283939 | 1024        | 0.01000     | 1.086     | 5.833        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.k_proj     | 0.00191071 | 1024        | 0.01000     | 0.325     | 2.941        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.v_proj     | 0.00179209 | 1024        | 0.01000     | 0.332     | 2.941        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.q_proj     | 0.00215087 | 1024        | 0.01000     | 0.329     | 2.941        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.o_proj     | 0.00120791 | 1024        | 0.01000     | 0.322     | 1.633        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | mlp.up_proj          | 0.00305059 | 1024        | 0.01000     | 0.344     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | mlp.gate_proj        | 0.00284500 | 1024        | 0.01000     | 0.347     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | mlp.down_proj        | 0.00381570 | 1024        | 0.01000     | 1.059     | 5.778        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.k_proj     | 0.00197454 | 1024        | 0.01000     | 0.326     | 2.912        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.v_proj     | 0.00213252 | 1024        | 0.01000     | 0.331     | 2.912        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.q_proj     | 0.00234417 | 1024        | 0.01000     | 0.325     | 2.912        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.o_proj     | 0.00125800 | 1024        | 0.01000     | 0.321     | 1.605        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | mlp.up_proj          | 0.00361909 | 1024        | 0.01000     | 0.343     | 2.274        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | mlp.gate_proj        | 0.00343414 | 1024        | 0.01000     | 0.343     | 2.274        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | mlp.down_proj        | 0.00499402 | 1024        | 0.01000     | 1.082     | 5.760        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.k_proj     | 0.00213576 | 1024        | 0.01000     | 0.323     | 2.910        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.v_proj     | 0.00248914 | 1024        | 0.01000     | 0.330     | 2.910        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.q_proj     | 0.00250225 | 1024        | 0.01000     | 0.323     | 2.910        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.o_proj     | 0.00149351 | 1024        | 0.01000     | 0.320     | 1.611        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | mlp.up_proj          | 0.00407900 | 1024        | 0.01000     | 0.347     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | mlp.gate_proj        | 0.00388730 | 1024        | 0.01000     | 0.352     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | mlp.down_proj        | 0.00597206 | 1024        | 0.01000     | 1.067     | 5.757        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.k_proj     | 0.00241553 | 1024        | 0.01000     | 0.326     | 2.913        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.v_proj     | 0.00278264 | 1024        | 0.01000     | 0.331     | 2.913        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.q_proj     | 0.00263978 | 1024        | 0.01000     | 0.325     | 2.913        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.o_proj     | 0.00114086 | 1024        | 0.01000     | 0.324     | 1.611        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | mlp.up_proj          | 0.00439268 | 1024        | 0.01000     | 0.347     | 2.264        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | mlp.gate_proj        | 0.00421358 | 1024        | 0.01000     | 0.347     | 2.264        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | mlp.down_proj        | 0.00652538 | 1024        | 0.01000     | 1.064     | 5.755        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.k_proj     | 0.00250822 | 1024        | 0.01000     | 0.330     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.v_proj     | 0.00292145 | 1024        | 0.01000     | 0.329     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.q_proj     | 0.00272720 | 1024        | 0.01000     | 0.328     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.o_proj     | 0.00166496 | 1024        | 0.01000     | 0.325     | 1.596        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | mlp.up_proj          | 0.00486483 | 1024        | 0.01000     | 0.345     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | mlp.gate_proj        | 0.00463757 | 1024        | 0.01000     | 0.345     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | mlp.down_proj        | 0.00763281 | 1024        | 0.01000     | 1.079     | 5.741        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.k_proj     | 0.00245900 | 1024        | 0.01000     | 0.327     | 2.918        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.v_proj     | 0.00309085 | 1024        | 0.01000     | 0.330     | 2.918        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.q_proj     | 0.00270033 | 1024        | 0.01000     | 0.325     | 2.918        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.o_proj     | 0.00172058 | 1024        | 0.01000     | 0.323     | 1.588        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | mlp.up_proj          | 0.00528648 | 1024        | 0.01000     | 0.344     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | mlp.gate_proj        | 0.00503584 | 1024        | 0.01000     | 0.346     | 2.268        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | mlp.down_proj        | 0.00821001 | 1024        | 0.01000     | 1.065     | 5.752        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.k_proj     | 0.00249569 | 1024        | 0.01000     | 0.322     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.v_proj     | 0.00336069 | 1024        | 0.01000     | 0.326     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.q_proj     | 0.00278166 | 1024        | 0.01000     | 0.324     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.o_proj     | 0.00178607 | 1024        | 0.01000     | 0.322     | 1.587        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | mlp.up_proj          | 0.00567806 | 1024        | 0.01000     | 0.344     | 2.269        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | mlp.gate_proj        | 0.00535870 | 1024        | 0.01000     | 0.345     | 2.269        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | mlp.down_proj        | 0.00909729 | 1024        | 0.01000     | 1.062     | 5.764        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.k_proj     | 0.00277396 | 1024        | 0.01000     | 0.321     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.v_proj     | 0.00383723 | 1024        | 0.01000     | 0.326     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.q_proj     | 0.00302633 | 1024        | 0.01000     | 0.321     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.o_proj     | 0.00154581 | 1024        | 0.01000     | 0.320     | 1.600        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | mlp.up_proj          | 0.00626509 | 1024        | 0.01000     | 0.342     | 2.258        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | mlp.gate_proj        | 0.00587403 | 1024        | 0.01000     | 0.344     | 2.258        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | mlp.down_proj        | 0.00994821 | 1024        | 0.01000     | 1.070     | 5.806        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.k_proj     | 0.00275949 | 1024        | 0.01000     | 0.323     | 2.928        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.v_proj     | 0.00403207 | 1024        | 0.01000     | 0.327     | 2.928        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.q_proj     | 0.00337218 | 1024        | 0.01000     | 0.343     | 2.928        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.o_proj     | 0.00285128 | 1024        | 0.01000     | 0.328     | 1.692        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | mlp.up_proj          | 0.00694099 | 1024        | 0.01000     | 0.345     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | mlp.gate_proj        | 0.00645662 | 1024        | 0.01000     | 0.350     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | mlp.down_proj        | 0.01157292 | 1024        | 0.01000     | 1.068     | 5.782        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.k_proj     | 0.00273842 | 1024        | 0.01000     | 0.326     | 2.931        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.v_proj     | 0.00409034 | 1024        | 0.01000     | 0.338     | 2.931        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.q_proj     | 0.00327006 | 1024        | 0.01000     | 0.335     | 2.931        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.o_proj     | 0.00237979 | 1024        | 0.01000     | 0.320     | 1.608        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | mlp.up_proj          | 0.00737265 | 1024        | 0.01000     | 0.348     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | mlp.gate_proj        | 0.00684825 | 1024        | 0.01000     | 0.349     | 2.267        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | mlp.down_proj        | 0.01203266 | 1024        | 0.01000     | 1.075     | 5.776        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.k_proj     | 0.00270947 | 1024        | 0.01000     | 0.325     | 2.913        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.v_proj     | 0.00397893 | 1024        | 0.01000     | 0.335     | 2.913        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.q_proj     | 0.00306799 | 1024        | 0.01000     | 0.328     | 2.913        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.o_proj     | 0.00292165 | 1024        | 0.01000     | 0.323     | 1.579        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | mlp.up_proj          | 0.00757896 | 1024        | 0.01000     | 0.353     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | mlp.gate_proj        | 0.00734318 | 1024        | 0.01000     | 0.352     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | mlp.down_proj        | 0.01195568 | 1024        | 0.01000     | 1.071     | 5.775        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.k_proj     | 0.00235838 | 1024        | 0.01000     | 0.324     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.v_proj     | 0.00336120 | 1024        | 0.01000     | 0.333     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.q_proj     | 0.00241587 | 1024        | 0.01000     | 0.327     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.o_proj     | 0.00320785 | 1024        | 0.01000     | 0.324     | 1.581        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | mlp.up_proj          | 0.00541829 | 1024        | 0.01000     | 0.345     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | mlp.gate_proj        | 0.00557940 | 1024        | 0.01000     | 0.348     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | mlp.down_proj        | 0.02216227 | 1024        | 0.01000     | 1.073     | 5.775        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.00001985', 'samples': '1024', 'damp': '0.01000', 'time': '0.539', 'fwd_time': '3.081'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00000295', 'samples': '1024', 'damp': '0.01000', 'time': '0.340', 'fwd_time': '3.081'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.00002787', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '3.081'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00000423', 'samples': '1024', 'damp': '0.01000', 'time': '0.318', 'fwd_time': '1.621'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.00003751', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.282'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.00006645', 'samples': '1024', 'damp': '0.01000', 'time': '0.340', 'fwd_time': '2.282'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.00000562', 'samples': '1024', 'damp': '0.01000', 'time': '1.064', 'fwd_time': '5.801'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.00005187', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.00001449', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.00005531', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00000887', 'samples': '1024', 'damp': '0.01000', 'time': '0.320', 'fwd_time': '1.596'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.00011060', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.258'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '0.00010949', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.258'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '0.00638598', 'samples': '1024', 'damp': '0.01000', 'time': '1.077', 'fwd_time': '5.783'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.00035380', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.00018597', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '0.00040798', 'samples': '1024', 'damp': '0.01000', 'time': '0.328', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00001671', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '1.601'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '0.00021604', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.257'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '0.00022287', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.257'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.00002792', 'samples': '1024', 'damp': '0.01000', 'time': '1.066', 'fwd_time': '5.771'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.00042073', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.00023468', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '0.00048230', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00002270', 'samples': '1024', 'damp': '0.01000', 'time': '0.320', 'fwd_time': '1.591'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '0.00031908', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.256'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '0.00033495', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.256'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.00005262', 'samples': '1024', 'damp': '0.01000', 'time': '1.063', 'fwd_time': '5.767'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.00075838', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.00039405', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '0.00085058', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00002860', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '1.605'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '0.00043644', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '0.00046541', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.00008456', 'samples': '1024', 'damp': '0.01000', 'time': '1.071', 'fwd_time': '5.786'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.00074804', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.00043334', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '0.00087107', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.00003989', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '1.582'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '0.00056745', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.255'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '0.00061597', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.255'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.00014951', 'samples': '1024', 'damp': '0.01000', 'time': '1.063', 'fwd_time': '5.774'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.00073265', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '2.909'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.00040081', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.909'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '0.00073878', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.909'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.00008366', 'samples': '1024', 'damp': '0.01000', 'time': '0.318', 'fwd_time': '1.581'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '0.00065809', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.262'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '0.00070560', 'samples': '1024', 'damp': '0.01000', 'time': '0.357', 'fwd_time': '2.262'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.00020356', 'samples': '1024', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '5.772'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.00088242', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.00050739', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '0.00088916', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.00010625', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '1.608'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '0.00082043', 'samples': '1024', 'damp': '0.01000', 'time': '0.351', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '0.00090248', 'samples': '1024', 'damp': '0.01000', 'time': '0.351', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.00031979', 'samples': '1024', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '5.790'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '0.00081696', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.00045344', 'samples': '1024', 'damp': '0.01000', 'time': '0.338', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '0.00083452', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.00014972', 'samples': '1024', 'damp': '0.01000', 'time': '0.331', 'fwd_time': '1.586'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '0.00093403', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '0.00099719', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.00041517', 'samples': '1024', 'damp': '0.01000', 'time': '1.074', 'fwd_time': '5.772'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.00108919', 'samples': '1024', 'damp': '0.01000', 'time': '0.329', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.00065300', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '0.00131218', 'samples': '1024', 'damp': '0.01000', 'time': '0.328', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.00020896', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '1.590'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '0.00108924', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.269'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '0.00114857', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.269'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.00052141', 'samples': '1024', 'damp': '0.01000', 'time': '1.067', 'fwd_time': '5.784'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.00112453', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.00072678', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '0.00128847', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.00023785', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '1.597'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '0.00121168', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '0.00124881', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.00065353', 'samples': '1024', 'damp': '0.01000', 'time': '1.068', 'fwd_time': '5.782'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '0.00128270', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.00083857', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '0.00153403', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.00029285', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '1.606'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '0.00135331', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '0.00136664', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.00074170', 'samples': '1024', 'damp': '0.01000', 'time': '1.074', 'fwd_time': '5.782'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '0.00137634', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.927'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.00087323', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.927'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '0.00147735', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.927'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.00031062', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '1.607'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '0.00149069', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.278'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '0.00146746', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.278'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.00088481', 'samples': '1024', 'damp': '0.01000', 'time': '1.072', 'fwd_time': '5.790'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '0.00152959', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.00107942', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '0.00174280', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.923'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.00037548', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '1.608'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '0.00161538', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '0.00155030', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.00104844', 'samples': '1024', 'damp': '0.01000', 'time': '1.074', 'fwd_time': '5.781'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '0.00143200', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.00100413', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '0.00155328', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.00064771', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '1.596'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '0.00173733', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '0.00163646', 'samples': '1024', 'damp': '0.01000', 'time': '0.353', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.00129538', 'samples': '1024', 'damp': '0.01000', 'time': '1.066', 'fwd_time': '5.778'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '0.00165415', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.00142532', 'samples': '1024', 'damp': '0.01000', 'time': '0.340', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '0.00187119', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.00063853', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '1.603'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '0.00198716', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '0.00186254', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.00169545', 'samples': '1024', 'damp': '0.01000', 'time': '1.069', 'fwd_time': '5.786'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '0.00178392', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.00159698', 'samples': '1024', 'damp': '0.01000', 'time': '0.338', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '0.00200559', 'samples': '1024', 'damp': '0.01000', 'time': '0.328', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.00063093', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '1.608'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '0.00229918', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '0.00215838', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '0.00212617', 'samples': '1024', 'damp': '0.01000', 'time': '1.067', 'fwd_time': '5.776'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '0.00176116', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.00154136', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '0.00198934', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.00091225', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '1.601'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '0.00261287', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.264'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '0.00245481', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.264'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '0.00283939', 'samples': '1024', 'damp': '0.01000', 'time': '1.086', 'fwd_time': '5.833'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.00191071', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.941'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.00179209', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '2.941'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '0.00215087', 'samples': '1024', 'damp': '0.01000', 'time': '0.329', 'fwd_time': '2.941'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.00120791', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '1.633'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '0.00305059', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '0.00284500', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '0.00381570', 'samples': '1024', 'damp': '0.01000', 'time': '1.059', 'fwd_time': '5.778'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '0.00197454', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.912'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.00213252', 'samples': '1024', 'damp': '0.01000', 'time': '0.331', 'fwd_time': '2.912'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '0.00234417', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.912'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '0.00125800', 'samples': '1024', 'damp': '0.01000', 'time': '0.321', 'fwd_time': '1.605'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '0.00361909', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.274'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '0.00343414', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.274'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '0.00499402', 'samples': '1024', 'damp': '0.01000', 'time': '1.082', 'fwd_time': '5.760'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.00213576', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '2.910'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.00248914', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '2.910'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '0.00250225', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '2.910'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '0.00149351', 'samples': '1024', 'damp': '0.01000', 'time': '0.320', 'fwd_time': '1.611'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '0.00407900', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '0.00388730', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '0.00597206', 'samples': '1024', 'damp': '0.01000', 'time': '1.067', 'fwd_time': '5.757'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.00241553', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.913'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.00278264', 'samples': '1024', 'damp': '0.01000', 'time': '0.331', 'fwd_time': '2.913'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '0.00263978', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.913'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '0.00114086', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '1.611'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '0.00439268', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.264'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '0.00421358', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.264'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '0.00652538', 'samples': '1024', 'damp': '0.01000', 'time': '1.064', 'fwd_time': '5.755'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '0.00250822', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '0.00292145', 'samples': '1024', 'damp': '0.01000', 'time': '0.329', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '0.00272720', 'samples': '1024', 'damp': '0.01000', 'time': '0.328', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.o_proj', 'loss': '0.00166496', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '1.596'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'mlp.up_proj', 'loss': '0.00486483', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'mlp.gate_proj', 'loss': '0.00463757', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'mlp.down_proj', 'loss': '0.00763281', 'samples': '1024', 'damp': '0.01000', 'time': '1.079', 'fwd_time': '5.741'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '0.00245900', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.918'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '0.00309085', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '2.918'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '0.00270033', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.918'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.o_proj', 'loss': '0.00172058', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '1.588'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'mlp.up_proj', 'loss': '0.00528648', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'mlp.gate_proj', 'loss': '0.00503584', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.268'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'mlp.down_proj', 'loss': '0.00821001', 'samples': '1024', 'damp': '0.01000', 'time': '1.065', 'fwd_time': '5.752'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.k_proj', 'loss': '0.00249569', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.v_proj', 'loss': '0.00336069', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.q_proj', 'loss': '0.00278166', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.o_proj', 'loss': '0.00178607', 'samples': '1024', 'damp': '0.01000', 'time': '0.322', 'fwd_time': '1.587'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'mlp.up_proj', 'loss': '0.00567806', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.269'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'mlp.gate_proj', 'loss': '0.00535870', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.269'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'mlp.down_proj', 'loss': '0.00909729', 'samples': '1024', 'damp': '0.01000', 'time': '1.062', 'fwd_time': '5.764'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.k_proj', 'loss': '0.00277396', 'samples': '1024', 'damp': '0.01000', 'time': '0.321', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.v_proj', 'loss': '0.00383723', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.q_proj', 'loss': '0.00302633', 'samples': '1024', 'damp': '0.01000', 'time': '0.321', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.o_proj', 'loss': '0.00154581', 'samples': '1024', 'damp': '0.01000', 'time': '0.320', 'fwd_time': '1.600'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'mlp.up_proj', 'loss': '0.00626509', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.258'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'mlp.gate_proj', 'loss': '0.00587403', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.258'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'mlp.down_proj', 'loss': '0.00994821', 'samples': '1024', 'damp': '0.01000', 'time': '1.070', 'fwd_time': '5.806'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.k_proj', 'loss': '0.00275949', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '2.928'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.v_proj', 'loss': '0.00403207', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.928'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.q_proj', 'loss': '0.00337218', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.928'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.o_proj', 'loss': '0.00285128', 'samples': '1024', 'damp': '0.01000', 'time': '0.328', 'fwd_time': '1.692'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'mlp.up_proj', 'loss': '0.00694099', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'mlp.gate_proj', 'loss': '0.00645662', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'mlp.down_proj', 'loss': '0.01157292', 'samples': '1024', 'damp': '0.01000', 'time': '1.068', 'fwd_time': '5.782'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.k_proj', 'loss': '0.00273842', 'samples': '1024', 'damp': '0.01000', 'time': '0.326', 'fwd_time': '2.931'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.v_proj', 'loss': '0.00409034', 'samples': '1024', 'damp': '0.01000', 'time': '0.338', 'fwd_time': '2.931'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.q_proj', 'loss': '0.00327006', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.931'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.o_proj', 'loss': '0.00237979', 'samples': '1024', 'damp': '0.01000', 'time': '0.320', 'fwd_time': '1.608'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'mlp.up_proj', 'loss': '0.00737265', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'mlp.gate_proj', 'loss': '0.00684825', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.267'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'mlp.down_proj', 'loss': '0.01203266', 'samples': '1024', 'damp': '0.01000', 'time': '1.075', 'fwd_time': '5.776'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.k_proj', 'loss': '0.00270947', 'samples': '1024', 'damp': '0.01000', 'time': '0.325', 'fwd_time': '2.913'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.v_proj', 'loss': '0.00397893', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.913'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.q_proj', 'loss': '0.00306799', 'samples': '1024', 'damp': '0.01000', 'time': '0.328', 'fwd_time': '2.913'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.o_proj', 'loss': '0.00292165', 'samples': '1024', 'damp': '0.01000', 'time': '0.323', 'fwd_time': '1.579'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'mlp.up_proj', 'loss': '0.00757896', 'samples': '1024', 'damp': '0.01000', 'time': '0.353', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'mlp.gate_proj', 'loss': '0.00734318', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'mlp.down_proj', 'loss': '0.01195568', 'samples': '1024', 'damp': '0.01000', 'time': '1.071', 'fwd_time': '5.775'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.k_proj', 'loss': '0.00235838', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.v_proj', 'loss': '0.00336120', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.q_proj', 'loss': '0.00241587', 'samples': '1024', 'damp': '0.01000', 'time': '0.327', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.o_proj', 'loss': '0.00320785', 'samples': '1024', 'damp': '0.01000', 'time': '0.324', 'fwd_time': '1.581'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'mlp.up_proj', 'loss': '0.00541829', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'mlp.gate_proj', 'loss': '0.00557940', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'mlp.down_proj', 'loss': '0.02216227', 'samples': '1024', 'damp': '0.01000', 'time': '1.073', 'fwd_time': '5.775'}
INFO  Packing model...
INFO  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`
INFO  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`
INFO  Kernel: candidates -> `[TritonV2QuantLinear, TorchQuantLinear]`
INFO  Kernel: selected -> `TritonV2QuantLinear`.
INFO  Model packed.
INFO  Format: Converting GPTQ v2 to v1
INFO  Saved Quantize Config:
{
  "bits": 8,
  "group_size": 128,
  "desc_act": true,
  "sym": true,
  "lm_head": false,
  "quant_method": "gptq",
  "checkpoint_format": "gptq",
  "pack_dtype": "int32",
  "meta": {
    "quantizer": [
      "gptqmodel:2.2.0"
    ],
    "uri": "https://github.com/modelcloud/gptqmodel",
    "damp_percent": 0.01,
    "damp_auto_increment": 0.0025,
    "static_groups": false,
    "true_sequential": true,
    "mse": 0.0
  }
}
Files in directory:
config.json
quant_log.csv
quantize_config.json
generation_config.json
Content of saved `generation_config.json`:
{
    "_from_model_config": true,
    "bos_token_id": 100000,
    "eos_token_id": 100001,
    "transformers_version": "4.51.3"
}
Content of saved `config.json`:
{
    "architectures": [
        "LlamaForCausalLM"
    ],
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 100000,
    "eos_token_id": 100001,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 11008,
    "max_position_embeddings": 4096,
    "mlp_bias": false,
    "model_type": "llama",
    "num_attention_heads": 32,
    "num_hidden_layers": 30,
    "num_key_value_heads": 32,
    "pretraining_tp": 1,
    "quantization_config": {
        "bits": 8,
        "checkpoint_format": "gptq",
        "desc_act": true,
        "group_size": 128,
        "lm_head": false,
        "meta": {
            "damp_auto_increment": 0.0025,
            "damp_percent": 0.01,
            "mse": 0.0,
            "quantizer": [
                "gptqmodel:2.2.0"
            ],
            "static_groups": false,
            "true_sequential": true,
            "uri": "https://github.com/modelcloud/gptqmodel"
        },
        "pack_dtype": "int32",
        "quant_method": "gptq",
        "sym": true
    },
    "rms_norm_eps": 1e-06,
    "rope_scaling": null,
    "rope_theta": 10000.0,
    "tie_word_embeddings": false,
    "torch_dtype": "bfloat16",
    "transformers_version": "4.51.3",
    "use_cache": true,
    "vocab_size": 102400
}
DEBUG Received safetensors_metadata: {'format': 'pt'}
INFO  Pre-Quantized model size: 13180.57MB, 12.87GB
INFO  Quantized model size: 7530.35MB, 7.35GB
INFO  Size difference: 5650.22MB, 5.52GB - 42.87%

Process finished with exit code 0
