INFO  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.
INFO  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.
Torch version: 2.5.1+cu121
CUDA Available: True
CUDA Version: 12.1
GPU Count: 1
GPU Name: NVIDIA GeForce RTX 4080
INFO  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]
INFO  Loader: Auto dtype (native bfloat16): `torch.bfloat16`
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 127.10it/s]
INFO  Model: Loaded `generation_config`: GenerationConfig {
  "bos_token_id": 100000,
  "eos_token_id": 100001
}

INFO  Kernel: loaded -> `[]`
INFO  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`
INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=100001 (token='<｜end▁of▁sentence｜>').
WARN  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 60.5517578125.
INFO  Process: progress logs for `gptq` will be streamed to file: `gptq_log_allodiality_time_05_17_2025_23h_14m_15s.log`
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.k_proj     | 0.00582200 | 1024        | 0.01000     | 0.583     | 3.083        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.v_proj     | 0.00084701 | 1024        | 0.01000     | 0.354     | 3.083        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.q_proj     | 0.00816230 | 1024        | 0.01000     | 0.341     | 3.083        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | self_attn.o_proj     | 0.00123634 | 1024        | 0.01000     | 0.332     | 1.655        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | mlp.up_proj          | 0.01104249 | 1024        | 0.01000     | 0.348     | 2.329        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | mlp.gate_proj        | 0.01681170 | 1024        | 0.01000     | 0.351     | 2.329        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 0         | mlp.down_proj        | 0.00163542 | 1024        | 0.01000     | 1.107     | 5.842        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.k_proj     | 0.01510683 | 1024        | 0.01000     | 0.335     | 2.946        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.v_proj     | 0.00414037 | 1024        | 0.01000     | 0.347     | 2.946        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.q_proj     | 0.01632623 | 1024        | 0.01000     | 0.342     | 2.946        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | self_attn.o_proj     | 0.00259195 | 1024        | 0.01000     | 0.342     | 1.608        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | mlp.up_proj          | 0.03245897 | 1024        | 0.01000     | 0.360     | 2.278        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | mlp.gate_proj        | 0.03218155 | 1024        | 0.01000     | 0.359     | 2.278        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 1         | mlp.down_proj        | 1.82459879 | 1024        | 0.01000     | 1.113     | 5.796        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.k_proj     | 0.10299703 | 1024        | 0.01000     | 0.337     | 2.939        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.v_proj     | 0.05440510 | 1024        | 0.01000     | 0.352     | 2.939        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.q_proj     | 0.11078355 | 1024        | 0.01000     | 0.339     | 2.939        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | self_attn.o_proj     | 0.00499729 | 1024        | 0.01000     | 0.335     | 1.622        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | mlp.up_proj          | 0.06335805 | 1024        | 0.01000     | 0.357     | 2.283        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | mlp.gate_proj        | 0.06543493 | 1024        | 0.01000     | 0.359     | 2.283        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 2         | mlp.down_proj        | 0.00824962 | 1024        | 0.01000     | 1.108     | 5.795        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.k_proj     | 0.12368236 | 1024        | 0.01000     | 0.335     | 2.940        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.v_proj     | 0.06888678 | 1024        | 0.01000     | 0.350     | 2.940        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.q_proj     | 0.13408825 | 1024        | 0.01000     | 0.347     | 2.940        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | self_attn.o_proj     | 0.00673703 | 1024        | 0.01000     | 0.336     | 1.622        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | mlp.up_proj          | 0.09359724 | 1024        | 0.01000     | 0.355     | 2.285        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | mlp.gate_proj        | 0.09818128 | 1024        | 0.01000     | 0.360     | 2.285        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 3         | mlp.down_proj        | 0.01548764 | 1024        | 0.01000     | 1.090     | 5.794        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.k_proj     | 0.22266926 | 1024        | 0.01000     | 0.346     | 2.941        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.v_proj     | 0.11556192 | 1024        | 0.01000     | 0.358     | 2.941        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.q_proj     | 0.23832682 | 1024        | 0.01000     | 0.342     | 2.941        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | self_attn.o_proj     | 0.00825900 | 1024        | 0.01000     | 0.336     | 1.624        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | mlp.up_proj          | 0.12778786 | 1024        | 0.01000     | 0.353     | 2.285        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | mlp.gate_proj        | 0.13647783 | 1024        | 0.01000     | 0.359     | 2.285        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 4         | mlp.down_proj        | 0.02451463 | 1024        | 0.01000     | 1.107     | 5.796        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.k_proj     | 0.21970284 | 1024        | 0.01000     | 0.335     | 2.937        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.v_proj     | 0.12711330 | 1024        | 0.01000     | 0.352     | 2.937        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.q_proj     | 0.24620157 | 1024        | 0.01000     | 0.339     | 2.937        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | self_attn.o_proj     | 0.01134281 | 1024        | 0.01000     | 0.336     | 1.621        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | mlp.up_proj          | 0.16621669 | 1024        | 0.01000     | 0.356     | 2.284        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | mlp.gate_proj        | 0.18058804 | 1024        | 0.01000     | 0.359     | 2.284        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 5         | mlp.down_proj        | 0.04293911 | 1024        | 0.01000     | 1.104     | 5.800        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.k_proj     | 0.21520750 | 1024        | 0.01000     | 0.335     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.v_proj     | 0.11760819 | 1024        | 0.01000     | 0.348     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.q_proj     | 0.21678042 | 1024        | 0.01000     | 0.340     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | self_attn.o_proj     | 0.02360092 | 1024        | 0.01000     | 0.340     | 1.621        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | mlp.up_proj          | 0.19346169 | 1024        | 0.01000     | 0.358     | 2.284        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | mlp.gate_proj        | 0.20787138 | 1024        | 0.01000     | 0.356     | 2.284        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 6         | mlp.down_proj        | 0.05912108 | 1024        | 0.01000     | 1.136     | 5.802        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.k_proj     | 0.25948632 | 1024        | 0.01000     | 0.336     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.v_proj     | 0.14879808 | 1024        | 0.01000     | 0.350     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.q_proj     | 0.26143247 | 1024        | 0.01000     | 0.345     | 2.926        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | self_attn.o_proj     | 0.03068669 | 1024        | 0.01000     | 0.333     | 1.607        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | mlp.up_proj          | 0.24073243 | 1024        | 0.01000     | 0.358     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | mlp.gate_proj        | 0.26534504 | 1024        | 0.01000     | 0.360     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 7         | mlp.down_proj        | 0.09302332 | 1024        | 0.01000     | 1.099     | 5.791        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.k_proj     | 0.24023581 | 1024        | 0.01000     | 0.333     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.v_proj     | 0.13295627 | 1024        | 0.01000     | 0.346     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.q_proj     | 0.24524301 | 1024        | 0.01000     | 0.339     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | self_attn.o_proj     | 0.04343276 | 1024        | 0.01000     | 0.331     | 1.586        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | mlp.up_proj          | 0.27397209 | 1024        | 0.01000     | 0.357     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | mlp.gate_proj        | 0.29293525 | 1024        | 0.01000     | 0.361     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 8         | mlp.down_proj        | 0.12091636 | 1024        | 0.01000     | 1.098     | 5.782        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.k_proj     | 0.32080543 | 1024        | 0.01000     | 0.336     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.v_proj     | 0.19185869 | 1024        | 0.01000     | 0.346     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.q_proj     | 0.38294670 | 1024        | 0.01000     | 0.339     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | self_attn.o_proj     | 0.06086418 | 1024        | 0.01000     | 0.333     | 1.596        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | mlp.up_proj          | 0.31975985 | 1024        | 0.01000     | 0.356     | 2.263        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | mlp.gate_proj        | 0.33737260 | 1024        | 0.01000     | 0.364     | 2.263        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 9         | mlp.down_proj        | 0.15179449 | 1024        | 0.01000     | 1.106     | 5.780        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.k_proj     | 0.33065778 | 1024        | 0.01000     | 0.335     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.v_proj     | 0.21333581 | 1024        | 0.01000     | 0.349     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.q_proj     | 0.37894124 | 1024        | 0.01000     | 0.342     | 2.920        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | self_attn.o_proj     | 0.06912826 | 1024        | 0.01000     | 0.333     | 1.598        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | mlp.up_proj          | 0.35577488 | 1024        | 0.01000     | 0.355     | 2.264        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | mlp.gate_proj        | 0.36739826 | 1024        | 0.01000     | 0.362     | 2.264        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 10        | mlp.down_proj        | 0.18956152 | 1024        | 0.01000     | 1.093     | 5.784        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.k_proj     | 0.37737116 | 1024        | 0.01000     | 0.335     | 2.931        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.v_proj     | 0.24628684 | 1024        | 0.01000     | 0.350     | 2.931        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.q_proj     | 0.44748187 | 1024        | 0.01000     | 0.343     | 2.931        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | self_attn.o_proj     | 0.08567642 | 1024        | 0.01000     | 0.335     | 1.602        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | mlp.up_proj          | 0.39708605 | 1024        | 0.01000     | 0.356     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | mlp.gate_proj        | 0.40118644 | 1024        | 0.01000     | 0.363     | 2.265        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 11        | mlp.down_proj        | 0.21623196 | 1024        | 0.01000     | 1.106     | 5.782        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.k_proj     | 0.40468448 | 1024        | 0.01000     | 0.338     | 2.927        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.v_proj     | 0.25675237 | 1024        | 0.01000     | 0.353     | 2.927        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.q_proj     | 0.43295345 | 1024        | 0.01000     | 0.342     | 2.927        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | self_attn.o_proj     | 0.09035677 | 1024        | 0.01000     | 0.337     | 1.603        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | mlp.up_proj          | 0.43783852 | 1024        | 0.01000     | 0.358     | 2.274        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | mlp.gate_proj        | 0.43115193 | 1024        | 0.01000     | 0.361     | 2.274        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 12        | mlp.down_proj        | 0.25760943 | 1024        | 0.01000     | 1.099     | 5.790        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.k_proj     | 0.44997290 | 1024        | 0.01000     | 0.336     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.v_proj     | 0.31731707 | 1024        | 0.01000     | 0.355     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.q_proj     | 0.51218301 | 1024        | 0.01000     | 0.337     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | self_attn.o_proj     | 0.10985103 | 1024        | 0.01000     | 0.335     | 1.600        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | mlp.up_proj          | 0.47431028 | 1024        | 0.01000     | 0.357     | 2.269        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | mlp.gate_proj        | 0.45539486 | 1024        | 0.01000     | 0.359     | 2.269        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 13        | mlp.down_proj        | 0.30612892 | 1024        | 0.01000     | 1.104     | 5.788        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.k_proj     | 0.42063940 | 1024        | 0.01000     | 0.335     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.v_proj     | 0.29464114 | 1024        | 0.01000     | 0.352     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.q_proj     | 0.45571738 | 1024        | 0.01000     | 0.339     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | self_attn.o_proj     | 0.19051291 | 1024        | 0.01000     | 0.333     | 1.601        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | mlp.up_proj          | 0.51025021 | 1024        | 0.01000     | 0.354     | 2.271        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | mlp.gate_proj        | 0.48064119 | 1024        | 0.01000     | 0.358     | 2.271        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 14        | mlp.down_proj        | 0.37993354 | 1024        | 0.01000     | 1.095     | 5.789        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.k_proj     | 0.48644912 | 1024        | 0.01000     | 0.334     | 2.939        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.v_proj     | 0.41856197 | 1024        | 0.01000     | 0.352     | 2.939        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.q_proj     | 0.54932880 | 1024        | 0.01000     | 0.342     | 2.939        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | self_attn.o_proj     | 0.18556923 | 1024        | 0.01000     | 0.338     | 1.602        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | mlp.up_proj          | 0.58343840 | 1024        | 0.01000     | 0.355     | 2.271        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | mlp.gate_proj        | 0.54740107 | 1024        | 0.01000     | 0.358     | 2.271        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 15        | mlp.down_proj        | 0.49746236 | 1024        | 0.01000     | 1.095     | 5.787        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.k_proj     | 0.52470279 | 1024        | 0.01000     | 0.339     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.v_proj     | 0.46879262 | 1024        | 0.01000     | 0.351     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.q_proj     | 0.58831495 | 1024        | 0.01000     | 0.336     | 2.936        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | self_attn.o_proj     | 0.18514913 | 1024        | 0.01000     | 0.330     | 1.605        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | mlp.up_proj          | 0.67573810 | 1024        | 0.01000     | 0.356     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | mlp.gate_proj        | 0.63478434 | 1024        | 0.01000     | 0.365     | 2.270        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 16        | mlp.down_proj        | 0.62424433 | 1024        | 0.01000     | 1.099     | 5.788        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.k_proj     | 0.51820910 | 1024        | 0.01000     | 0.339     | 2.975        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.v_proj     | 0.45266068 | 1024        | 0.01000     | 0.356     | 2.975        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.q_proj     | 0.58437687 | 1024        | 0.01000     | 0.346     | 2.975        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | self_attn.o_proj     | 0.26534677 | 1024        | 0.01000     | 0.335     | 1.607        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | mlp.up_proj          | 0.76768583 | 1024        | 0.01000     | 0.358     | 2.287        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | mlp.gate_proj        | 0.72103012 | 1024        | 0.01000     | 0.366     | 2.287        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 17        | mlp.down_proj        | 0.82909822 | 1024        | 0.01000     | 1.123     | 5.810        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.k_proj     | 0.56165600 | 1024        | 0.01000     | 0.334     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.v_proj     | 0.52632344 | 1024        | 0.01000     | 0.348     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.q_proj     | 0.63011861 | 1024        | 0.01000     | 0.346     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | self_attn.o_proj     | 0.34936097 | 1024        | 0.01000     | 0.334     | 1.580        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | mlp.up_proj          | 0.89535224 | 1024        | 0.01000     | 0.350     | 2.252        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | mlp.gate_proj        | 0.83570075 | 1024        | 0.01000     | 0.358     | 2.252        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 18        | mlp.down_proj        | 1.11535525 | 1024        | 0.01000     | 1.094     | 5.756        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.k_proj     | 0.57972705 | 1024        | 0.01000     | 0.334     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.v_proj     | 0.62538081 | 1024        | 0.01000     | 0.344     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.q_proj     | 0.68517768 | 1024        | 0.01000     | 0.343     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | self_attn.o_proj     | 0.36597785 | 1024        | 0.01000     | 0.337     | 1.582        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | mlp.up_proj          | 1.06170499 | 1024        | 0.01000     | 0.350     | 2.252        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | mlp.gate_proj        | 1.00870609 | 1024        | 0.01000     | 0.354     | 2.252        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 19        | mlp.down_proj        | 1.45825994 | 1024        | 0.01000     | 1.092     | 5.755        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.k_proj     | 0.62678647 | 1024        | 0.01000     | 0.333     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.v_proj     | 0.73008287 | 1024        | 0.01000     | 0.347     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.q_proj     | 0.73291880 | 1024        | 0.01000     | 0.335     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | self_attn.o_proj     | 0.43338850 | 1024        | 0.01000     | 0.332     | 1.582        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | mlp.up_proj          | 1.19692373 | 1024        | 0.01000     | 0.352     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | mlp.gate_proj        | 1.14282918 | 1024        | 0.01000     | 0.358     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 20        | mlp.down_proj        | 1.74870801 | 1024        | 0.01000     | 1.096     | 5.757        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.k_proj     | 0.70866740 | 1024        | 0.01000     | 0.334     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.v_proj     | 0.81655383 | 1024        | 0.01000     | 0.343     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.q_proj     | 0.76085645 | 1024        | 0.01000     | 0.336     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | self_attn.o_proj     | 0.33327019 | 1024        | 0.01000     | 0.336     | 1.580        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | mlp.up_proj          | 1.28917432 | 1024        | 0.01000     | 0.353     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | mlp.gate_proj        | 1.23596025 | 1024        | 0.01000     | 0.355     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 21        | mlp.down_proj        | 1.91064596 | 1024        | 0.01000     | 1.093     | 5.752        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.k_proj     | 0.73690844 | 1024        | 0.01000     | 0.333     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.v_proj     | 0.85702038 | 1024        | 0.01000     | 0.345     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.q_proj     | 0.79150414 | 1024        | 0.01000     | 0.334     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | self_attn.o_proj     | 0.48437089 | 1024        | 0.01000     | 0.336     | 1.585        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | mlp.up_proj          | 1.42688525 | 1024        | 0.01000     | 0.356     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | mlp.gate_proj        | 1.36021292 | 1024        | 0.01000     | 0.357     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 22        | mlp.down_proj        | 2.23810315 | 1024        | 0.01000     | 1.104     | 5.751        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.k_proj     | 0.72193003 | 1024        | 0.01000     | 0.344     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.v_proj     | 0.90645432 | 1024        | 0.01000     | 0.356     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.q_proj     | 0.77836865 | 1024        | 0.01000     | 0.347     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | self_attn.o_proj     | 0.50144243 | 1024        | 0.01000     | 0.333     | 1.579        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | mlp.up_proj          | 1.55046105 | 1024        | 0.01000     | 0.358     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | mlp.gate_proj        | 1.47670400 | 1024        | 0.01000     | 0.354     | 2.253        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 23        | mlp.down_proj        | 2.41171551 | 1024        | 0.01000     | 1.094     | 5.752        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.k_proj     | 0.73235279 | 1024        | 0.01000     | 0.335     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.v_proj     | 0.98626238 | 1024        | 0.01000     | 0.344     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.q_proj     | 0.80623692 | 1024        | 0.01000     | 0.334     | 2.919        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | self_attn.o_proj     | 0.52146739 | 1024        | 0.01000     | 0.332     | 1.584        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | mlp.up_proj          | 1.66434741 | 1024        | 0.01000     | 0.353     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | mlp.gate_proj        | 1.57049775 | 1024        | 0.01000     | 0.355     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 24        | mlp.down_proj        | 2.66621399 | 1024        | 0.01000     | 1.098     | 5.752        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.k_proj     | 0.81401390 | 1024        | 0.01000     | 0.336     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.v_proj     | 1.12572110 | 1024        | 0.01000     | 0.344     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.q_proj     | 0.88017702 | 1024        | 0.01000     | 0.343     | 2.915        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | self_attn.o_proj     | 0.45171222 | 1024        | 0.01000     | 0.334     | 1.582        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | mlp.up_proj          | 1.83822644 | 1024        | 0.01000     | 0.352     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | mlp.gate_proj        | 1.72364604 | 1024        | 0.01000     | 0.354     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 25        | mlp.down_proj        | 2.92838454 | 1024        | 0.01000     | 1.099     | 5.754        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.k_proj     | 0.81041360 | 1024        | 0.01000     | 0.336     | 2.918        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.v_proj     | 1.18372309 | 1024        | 0.01000     | 0.344     | 2.918        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.q_proj     | 0.98293185 | 1024        | 0.01000     | 0.337     | 2.918        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | self_attn.o_proj     | 0.83169144 | 1024        | 0.01000     | 0.334     | 1.583        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | mlp.up_proj          | 2.03534317 | 1024        | 0.01000     | 0.356     | 2.252        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | mlp.gate_proj        | 1.89290643 | 1024        | 0.01000     | 0.359     | 2.252        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 26        | mlp.down_proj        | 3.40075040 | 1024        | 0.01000     | 1.098     | 5.757        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.k_proj     | 0.80365252 | 1024        | 0.01000     | 0.342     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.v_proj     | 1.20044792 | 1024        | 0.01000     | 0.344     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.q_proj     | 0.95055467 | 1024        | 0.01000     | 0.343     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | self_attn.o_proj     | 0.69349045 | 1024        | 0.01000     | 0.331     | 1.580        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | mlp.up_proj          | 2.16433716 | 1024        | 0.01000     | 0.355     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | mlp.gate_proj        | 2.01157570 | 1024        | 0.01000     | 0.358     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 27        | mlp.down_proj        | 3.54607034 | 1024        | 0.01000     | 1.101     | 5.757        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.k_proj     | 0.79577541 | 1024        | 0.01000     | 0.334     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.v_proj     | 1.16849113 | 1024        | 0.01000     | 0.347     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.q_proj     | 0.89717233 | 1024        | 0.01000     | 0.336     | 2.916        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | self_attn.o_proj     | 0.85309970 | 1024        | 0.01000     | 0.334     | 1.581        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | mlp.up_proj          | 2.21625543 | 1024        | 0.01000     | 0.357     | 2.256        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | mlp.gate_proj        | 2.15048075 | 1024        | 0.01000     | 0.358     | 2.256        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 28        | mlp.down_proj        | 3.50135851 | 1024        | 0.01000     | 1.103     | 5.752        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |
INFO  --------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.k_proj     | 0.69501579 | 1024        | 0.01000     | 0.331     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.v_proj     | 0.99326634 | 1024        | 0.01000     | 0.348     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.q_proj     | 0.72452557 | 1024        | 0.01000     | 0.345     | 2.914        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | self_attn.o_proj     | 0.93284798 | 1024        | 0.01000     | 0.345     | 1.581        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | mlp.up_proj          | 1.60297370 | 1024        | 0.01000     | 0.355     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | mlp.gate_proj        | 1.64992905 | 1024        | 0.01000     | 0.360     | 2.251        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  | gptq        | 29        | mlp.down_proj        | 6.42152023 | 1024        | 0.01000     | 1.088     | 5.749        |
INFO  --------------------------------------------------------------------------------------------------------------------------------
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.00582200', 'samples': '1024', 'damp': '0.01000', 'time': '0.583', 'fwd_time': '3.083'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00084701', 'samples': '1024', 'damp': '0.01000', 'time': '0.354', 'fwd_time': '3.083'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.00816230', 'samples': '1024', 'damp': '0.01000', 'time': '0.341', 'fwd_time': '3.083'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00123634', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '1.655'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.01104249', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.329'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.01681170', 'samples': '1024', 'damp': '0.01000', 'time': '0.351', 'fwd_time': '2.329'}
INFO  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.00163542', 'samples': '1024', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '5.842'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.01510683', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.946'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.00414037', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.946'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.01632623', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.946'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00259195', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '1.608'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.03245897', 'samples': '1024', 'damp': '0.01000', 'time': '0.360', 'fwd_time': '2.278'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '0.03218155', 'samples': '1024', 'damp': '0.01000', 'time': '0.359', 'fwd_time': '2.278'}
INFO  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '1.82459879', 'samples': '1024', 'damp': '0.01000', 'time': '1.113', 'fwd_time': '5.796'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.10299703', 'samples': '1024', 'damp': '0.01000', 'time': '0.337', 'fwd_time': '2.939'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.05440510', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.939'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '0.11078355', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.939'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00499729', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '1.622'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '0.06335805', 'samples': '1024', 'damp': '0.01000', 'time': '0.357', 'fwd_time': '2.283'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '0.06543493', 'samples': '1024', 'damp': '0.01000', 'time': '0.359', 'fwd_time': '2.283'}
INFO  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.00824962', 'samples': '1024', 'damp': '0.01000', 'time': '1.108', 'fwd_time': '5.795'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.12368236', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.940'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.06888678', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.940'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '0.13408825', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.940'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00673703', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '1.622'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '0.09359724', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.285'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '0.09818128', 'samples': '1024', 'damp': '0.01000', 'time': '0.360', 'fwd_time': '2.285'}
INFO  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.01548764', 'samples': '1024', 'damp': '0.01000', 'time': '1.090', 'fwd_time': '5.794'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.22266926', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.941'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.11556192', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.941'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '0.23832682', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.941'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00825900', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '1.624'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '0.12778786', 'samples': '1024', 'damp': '0.01000', 'time': '0.353', 'fwd_time': '2.285'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '0.13647783', 'samples': '1024', 'damp': '0.01000', 'time': '0.359', 'fwd_time': '2.285'}
INFO  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.02451463', 'samples': '1024', 'damp': '0.01000', 'time': '1.107', 'fwd_time': '5.796'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.21970284', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.937'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.12711330', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.937'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '0.24620157', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.937'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.01134281', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '1.621'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '0.16621669', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.284'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '0.18058804', 'samples': '1024', 'damp': '0.01000', 'time': '0.359', 'fwd_time': '2.284'}
INFO  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.04293911', 'samples': '1024', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '5.800'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.21520750', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.11760819', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '0.21678042', 'samples': '1024', 'damp': '0.01000', 'time': '0.340', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.02360092', 'samples': '1024', 'damp': '0.01000', 'time': '0.340', 'fwd_time': '1.621'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '0.19346169', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.284'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '0.20787138', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.284'}
INFO  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.05912108', 'samples': '1024', 'damp': '0.01000', 'time': '1.136', 'fwd_time': '5.802'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.25948632', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.14879808', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '0.26143247', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.926'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.03068669', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '1.607'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '0.24073243', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '0.26534504', 'samples': '1024', 'damp': '0.01000', 'time': '0.360', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.09302332', 'samples': '1024', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '5.791'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '0.24023581', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.13295627', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '0.24524301', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.04343276', 'samples': '1024', 'damp': '0.01000', 'time': '0.331', 'fwd_time': '1.586'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '0.27397209', 'samples': '1024', 'damp': '0.01000', 'time': '0.357', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '0.29293525', 'samples': '1024', 'damp': '0.01000', 'time': '0.361', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.12091636', 'samples': '1024', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '5.782'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.32080543', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.19185869', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '0.38294670', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.06086418', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '1.596'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '0.31975985', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.263'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '0.33737260', 'samples': '1024', 'damp': '0.01000', 'time': '0.364', 'fwd_time': '2.263'}
INFO  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.15179449', 'samples': '1024', 'damp': '0.01000', 'time': '1.106', 'fwd_time': '5.780'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.33065778', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.21333581', 'samples': '1024', 'damp': '0.01000', 'time': '0.349', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '0.37894124', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.920'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.06912826', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '1.598'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '0.35577488', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.264'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '0.36739826', 'samples': '1024', 'damp': '0.01000', 'time': '0.362', 'fwd_time': '2.264'}
INFO  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.18956152', 'samples': '1024', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '5.784'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '0.37737116', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.931'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.24628684', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.931'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '0.44748187', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.931'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.08567642', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '1.602'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '0.39708605', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '0.40118644', 'samples': '1024', 'damp': '0.01000', 'time': '0.363', 'fwd_time': '2.265'}
INFO  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.21623196', 'samples': '1024', 'damp': '0.01000', 'time': '1.106', 'fwd_time': '5.782'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '0.40468448', 'samples': '1024', 'damp': '0.01000', 'time': '0.338', 'fwd_time': '2.927'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.25675237', 'samples': '1024', 'damp': '0.01000', 'time': '0.353', 'fwd_time': '2.927'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '0.43295345', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.927'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.09035677', 'samples': '1024', 'damp': '0.01000', 'time': '0.337', 'fwd_time': '1.603'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '0.43783852', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.274'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '0.43115193', 'samples': '1024', 'damp': '0.01000', 'time': '0.361', 'fwd_time': '2.274'}
INFO  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.25760943', 'samples': '1024', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '5.790'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '0.44997290', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.31731707', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '0.51218301', 'samples': '1024', 'damp': '0.01000', 'time': '0.337', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.10985103', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '1.600'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '0.47431028', 'samples': '1024', 'damp': '0.01000', 'time': '0.357', 'fwd_time': '2.269'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '0.45539486', 'samples': '1024', 'damp': '0.01000', 'time': '0.359', 'fwd_time': '2.269'}
INFO  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.30612892', 'samples': '1024', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '5.788'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '0.42063940', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.29464114', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '0.45571738', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.19051291', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '1.601'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '0.51025021', 'samples': '1024', 'damp': '0.01000', 'time': '0.354', 'fwd_time': '2.271'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '0.48064119', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.271'}
INFO  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.37993354', 'samples': '1024', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '5.789'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '0.48644912', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.939'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.41856197', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.939'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '0.54932880', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.939'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.18556923', 'samples': '1024', 'damp': '0.01000', 'time': '0.338', 'fwd_time': '1.602'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '0.58343840', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.271'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '0.54740107', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.271'}
INFO  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.49746236', 'samples': '1024', 'damp': '0.01000', 'time': '1.095', 'fwd_time': '5.787'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '0.52470279', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.46879262', 'samples': '1024', 'damp': '0.01000', 'time': '0.351', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '0.58831495', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.936'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.18514913', 'samples': '1024', 'damp': '0.01000', 'time': '0.330', 'fwd_time': '1.605'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '0.67573810', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '0.63478434', 'samples': '1024', 'damp': '0.01000', 'time': '0.365', 'fwd_time': '2.270'}
INFO  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '0.62424433', 'samples': '1024', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '5.788'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '0.51820910', 'samples': '1024', 'damp': '0.01000', 'time': '0.339', 'fwd_time': '2.975'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.45266068', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.975'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '0.58437687', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.975'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.26534677', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '1.607'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '0.76768583', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.287'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '0.72103012', 'samples': '1024', 'damp': '0.01000', 'time': '0.366', 'fwd_time': '2.287'}
INFO  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '0.82909822', 'samples': '1024', 'damp': '0.01000', 'time': '1.123', 'fwd_time': '5.810'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.56165600', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.52632344', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '0.63011861', 'samples': '1024', 'damp': '0.01000', 'time': '0.346', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.34936097', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '1.580'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '0.89535224', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.252'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '0.83570075', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.252'}
INFO  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '1.11535525', 'samples': '1024', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '5.756'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '0.57972705', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.62538081', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '0.68517768', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '0.36597785', 'samples': '1024', 'damp': '0.01000', 'time': '0.337', 'fwd_time': '1.582'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '1.06170499', 'samples': '1024', 'damp': '0.01000', 'time': '0.350', 'fwd_time': '2.252'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '1.00870609', 'samples': '1024', 'damp': '0.01000', 'time': '0.354', 'fwd_time': '2.252'}
INFO  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '1.45825994', 'samples': '1024', 'damp': '0.01000', 'time': '1.092', 'fwd_time': '5.755'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.62678647', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.73008287', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '0.73291880', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '0.43338850', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '1.582'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '1.19692373', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '1.14282918', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '1.74870801', 'samples': '1024', 'damp': '0.01000', 'time': '1.096', 'fwd_time': '5.757'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.70866740', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.81655383', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '0.76085645', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '0.33327019', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '1.580'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '1.28917432', 'samples': '1024', 'damp': '0.01000', 'time': '0.353', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '1.23596025', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '1.91064596', 'samples': '1024', 'damp': '0.01000', 'time': '1.093', 'fwd_time': '5.752'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '0.73690844', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '0.85702038', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '0.79150414', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.o_proj', 'loss': '0.48437089', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '1.585'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'mlp.up_proj', 'loss': '1.42688525', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'mlp.gate_proj', 'loss': '1.36021292', 'samples': '1024', 'damp': '0.01000', 'time': '0.357', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 22, 'module': 'mlp.down_proj', 'loss': '2.23810315', 'samples': '1024', 'damp': '0.01000', 'time': '1.104', 'fwd_time': '5.751'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '0.72193003', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '0.90645432', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '0.77836865', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.o_proj', 'loss': '0.50144243', 'samples': '1024', 'damp': '0.01000', 'time': '0.333', 'fwd_time': '1.579'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'mlp.up_proj', 'loss': '1.55046105', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'mlp.gate_proj', 'loss': '1.47670400', 'samples': '1024', 'damp': '0.01000', 'time': '0.354', 'fwd_time': '2.253'}
INFO  {'process': 'gptq', 'layer': 23, 'module': 'mlp.down_proj', 'loss': '2.41171551', 'samples': '1024', 'damp': '0.01000', 'time': '1.094', 'fwd_time': '5.752'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.k_proj', 'loss': '0.73235279', 'samples': '1024', 'damp': '0.01000', 'time': '0.335', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.v_proj', 'loss': '0.98626238', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.q_proj', 'loss': '0.80623692', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.919'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'self_attn.o_proj', 'loss': '0.52146739', 'samples': '1024', 'damp': '0.01000', 'time': '0.332', 'fwd_time': '1.584'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'mlp.up_proj', 'loss': '1.66434741', 'samples': '1024', 'damp': '0.01000', 'time': '0.353', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'mlp.gate_proj', 'loss': '1.57049775', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 24, 'module': 'mlp.down_proj', 'loss': '2.66621399', 'samples': '1024', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '5.752'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.k_proj', 'loss': '0.81401390', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.v_proj', 'loss': '1.12572110', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.q_proj', 'loss': '0.88017702', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.915'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'self_attn.o_proj', 'loss': '0.45171222', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '1.582'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'mlp.up_proj', 'loss': '1.83822644', 'samples': '1024', 'damp': '0.01000', 'time': '0.352', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'mlp.gate_proj', 'loss': '1.72364604', 'samples': '1024', 'damp': '0.01000', 'time': '0.354', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 25, 'module': 'mlp.down_proj', 'loss': '2.92838454', 'samples': '1024', 'damp': '0.01000', 'time': '1.099', 'fwd_time': '5.754'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.k_proj', 'loss': '0.81041360', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.918'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.v_proj', 'loss': '1.18372309', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.918'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.q_proj', 'loss': '0.98293185', 'samples': '1024', 'damp': '0.01000', 'time': '0.337', 'fwd_time': '2.918'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'self_attn.o_proj', 'loss': '0.83169144', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '1.583'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'mlp.up_proj', 'loss': '2.03534317', 'samples': '1024', 'damp': '0.01000', 'time': '0.356', 'fwd_time': '2.252'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'mlp.gate_proj', 'loss': '1.89290643', 'samples': '1024', 'damp': '0.01000', 'time': '0.359', 'fwd_time': '2.252'}
INFO  {'process': 'gptq', 'layer': 26, 'module': 'mlp.down_proj', 'loss': '3.40075040', 'samples': '1024', 'damp': '0.01000', 'time': '1.098', 'fwd_time': '5.757'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.k_proj', 'loss': '0.80365252', 'samples': '1024', 'damp': '0.01000', 'time': '0.342', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.v_proj', 'loss': '1.20044792', 'samples': '1024', 'damp': '0.01000', 'time': '0.344', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.q_proj', 'loss': '0.95055467', 'samples': '1024', 'damp': '0.01000', 'time': '0.343', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'self_attn.o_proj', 'loss': '0.69349045', 'samples': '1024', 'damp': '0.01000', 'time': '0.331', 'fwd_time': '1.580'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'mlp.up_proj', 'loss': '2.16433716', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'mlp.gate_proj', 'loss': '2.01157570', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 27, 'module': 'mlp.down_proj', 'loss': '3.54607034', 'samples': '1024', 'damp': '0.01000', 'time': '1.101', 'fwd_time': '5.757'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.k_proj', 'loss': '0.79577541', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.v_proj', 'loss': '1.16849113', 'samples': '1024', 'damp': '0.01000', 'time': '0.347', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.q_proj', 'loss': '0.89717233', 'samples': '1024', 'damp': '0.01000', 'time': '0.336', 'fwd_time': '2.916'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'self_attn.o_proj', 'loss': '0.85309970', 'samples': '1024', 'damp': '0.01000', 'time': '0.334', 'fwd_time': '1.581'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'mlp.up_proj', 'loss': '2.21625543', 'samples': '1024', 'damp': '0.01000', 'time': '0.357', 'fwd_time': '2.256'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'mlp.gate_proj', 'loss': '2.15048075', 'samples': '1024', 'damp': '0.01000', 'time': '0.358', 'fwd_time': '2.256'}
INFO  {'process': 'gptq', 'layer': 28, 'module': 'mlp.down_proj', 'loss': '3.50135851', 'samples': '1024', 'damp': '0.01000', 'time': '1.103', 'fwd_time': '5.752'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.k_proj', 'loss': '0.69501579', 'samples': '1024', 'damp': '0.01000', 'time': '0.331', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.v_proj', 'loss': '0.99326634', 'samples': '1024', 'damp': '0.01000', 'time': '0.348', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.q_proj', 'loss': '0.72452557', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '2.914'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'self_attn.o_proj', 'loss': '0.93284798', 'samples': '1024', 'damp': '0.01000', 'time': '0.345', 'fwd_time': '1.581'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'mlp.up_proj', 'loss': '1.60297370', 'samples': '1024', 'damp': '0.01000', 'time': '0.355', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'mlp.gate_proj', 'loss': '1.64992905', 'samples': '1024', 'damp': '0.01000', 'time': '0.360', 'fwd_time': '2.251'}
INFO  {'process': 'gptq', 'layer': 29, 'module': 'mlp.down_proj', 'loss': '6.42152023', 'samples': '1024', 'damp': '0.01000', 'time': '1.088', 'fwd_time': '5.749'}
INFO  Packing model...
INFO  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`
INFO  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`
INFO  Kernel: candidates -> `[TritonV2QuantLinear, TorchQuantLinear]`
INFO  Kernel: selected -> `TritonV2QuantLinear`.
INFO  Model packed.
INFO  Format: Converting GPTQ v2 to v1
INFO  Saved Quantize Config:
{
  "bits": 4,
  "group_size": 128,
  "desc_act": true,
  "sym": true,
  "lm_head": false,
  "quant_method": "gptq",
  "checkpoint_format": "gptq",
  "pack_dtype": "int32",
  "meta": {
    "quantizer": [
      "gptqmodel:2.2.0"
    ],
    "uri": "https://github.com/modelcloud/gptqmodel",
    "damp_percent": 0.01,
    "damp_auto_increment": 0.0025,
    "static_groups": false,
    "true_sequential": true,
    "mse": 0.0
  }
}
Files in directory:
config.json
quant_log.csv
quantize_config.json
generation_config.json
Content of saved `generation_config.json`:
{
    "_from_model_config": true,
    "bos_token_id": 100000,
    "eos_token_id": 100001,
    "transformers_version": "4.51.3"
}
Content of saved `config.json`:
{
    "architectures": [
        "LlamaForCausalLM"
    ],
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 100000,
    "eos_token_id": 100001,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 11008,
    "max_position_embeddings": 4096,
    "mlp_bias": false,
    "model_type": "llama",
    "num_attention_heads": 32,
    "num_hidden_layers": 30,
    "num_key_value_heads": 32,
    "pretraining_tp": 1,
    "quantization_config": {
        "bits": 4,
        "checkpoint_format": "gptq",
        "desc_act": true,
        "group_size": 128,
        "lm_head": false,
        "meta": {
            "damp_auto_increment": 0.0025,
            "damp_percent": 0.01,
            "mse": 0.0,
            "quantizer": [
                "gptqmodel:2.2.0"
            ],
            "static_groups": false,
            "true_sequential": true,
            "uri": "https://github.com/modelcloud/gptqmodel"
        },
        "pack_dtype": "int32",
        "quant_method": "gptq",
        "sym": true
    },
    "rms_norm_eps": 1e-06,
    "rope_scaling": null,
    "rope_theta": 10000.0,
    "tie_word_embeddings": false,
    "torch_dtype": "bfloat16",
    "transformers_version": "4.51.3",
    "use_cache": true,
    "vocab_size": 102400
}
DEBUG Received safetensors_metadata: {'format': 'pt'}
INFO  Pre-Quantized model size: 13180.57MB, 12.87GB
INFO  Quantized model size: 4612.73MB, 4.50GB
INFO  Size difference: 8567.83MB, 8.37GB - 65.00%

Process finished with exit code 0
